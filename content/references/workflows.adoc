[[cdWorkflows]]
=== Workflow Reference Guide

Many of these Architecture notes explain the sequence of steps in various parts of the codebase. It is not necessary to understand these processes in order to use Cloudera-Deploy for Deployments, but if you want to make your own processes they may be quite informative.

==== quickstart.sh

https://github.com/cloudera-labs/cloudera-deploy/blob/main/quickstart.sh[Quickstart.sh] is provided in the root of the Cloudera-Deploy repository, itself being the reference entrypoint to the tooling. It is intended to prepare and launch you into a shell running inside the Docker execution container where all the necessary software is already prepared for.

You'll know you are in this custom shell by the Orange coloured `cldr <version> #>` prompt.

.When you invoke quickstart.sh in Cloudera-Deploy, here is the general sequence of steps

. It determines the parent directory of quickstart.sh, and sets that as the Project directory unless the user has passed in a variable for this purpose
. Checks if Docker is running
. Runs ‘docker pull’ against the image if there is an updated image available
. Creates the default local User Profile mount paths on the local machine if not present
. Creates a default Cloudera Deploy Profile in the default path if not present
. Checks if various development helper options are set:
.. If `CLDR_COLLECTION_PATH` is set, that path is put into the ANSIBLE_COLLECTIONS_PATH, and the user is instructed to use /runner/project as the base path instead of /opt/
.. If `CLDR_PYTHON_PATH` is set, then that path is added to the PYTHON_PATH in the internal container
. Checks if ssh-agent and ssh-auth-sock are running / available
. Then:
.. If the Runner container is present and running, a new bash session is started
.. If the Runner container is present but stopped, it is removed
.. If the Runner container is not present or removed by the previous step, a new one is instantiated and a bash session started
. The current release version of cloudera-deploy is cloned into /opt/cloudera-deploy alongside the other Ansible Collection pieces
. The user is given several notices and dropped into an orange coloured bash prompt in the /runner directory

IMPORTANT: The only way to change the /runner/project directory mounted into the container is to stop the container and rerun the quickstart with a different path as the argument.

NOTE: The Ansible Log is pointed at ~/.config/cloudera-deploy/log by the quickstart container launcher

==== Main Playbook in Cloudera-Deploy

The https://github.com/cloudera-labs/cloudera-deploy/blob/main/main.yml[main.yml] Playbook is also in the root of Cloudera-Deploy and is intended as the entrypoint that you would call with the `ansible-playbook` command in the orange bash prompt we provide you in the Quickstart.

IMPORTANT: If you invoke Ansible in a directory other than /runner within cldr-runner, behavior will change as Ansible checks for particular directories relative to the execution path and uses that to find some defaults. Yes this is often annoying, they seem resistant to changing it.

.The main Playbook is generally invoked from one of two locations:

*/opt/cloudera-deploy/main.yml*  +
This is the current release version of Cloudera-Deploy baked into the Runner itself

*/runner/project/cloudera-deploy/main.yml*  +
If you have launched quickstart.sh following the usual process, then this is the version of Cloudera-Deploy you have checked out from Github on your local machine, as your Project directory is mounted to /runner/project, and thus cloudera-deploy will be in that project directory.

If you are editing the Playbooks or Collections, you should launch from /runner/project to pick up your changes.

.Here is the general sequence of steps:

. Ansible initialisation
.. /runner/env/envvars are loaded
.. Any Ansible Inventory in /runner/inventory is read
. The init Role is run
.. Run Metadata is immediately logged
.. The Definition Path and Files are validated
.. The User Profile is included, making it the lowest in Definition precedence
.. We validate that an Admin Password has been supplied
.. We examine the presence and controls around definition.yml and cluster.yml
.. definition.yml is included, making it the second in precedence
.. The selected cluster.yml is included (if present), making it third and highest in precedence for Definition files
.. ‘globals’ is then generated from general top level facts found in files included thus far
.. ‘globals’ is then updated by any values explicitly set as ‘globals’ from definition.yml, making this the highest definition authority only trumped by extra vars following the usual Ansible hierarchy
.. The Namespace is then validated and set
*** Note that this is not a namespace in the sense of Kubernetes, but the name_prefix that is used for all resources generated by Cloudera-Deploy
.. SSH keys are then validated, if necessary created, and then set
.. If a Dynamic Inventory is found, it is then validated and parsed
.. If the Download Mirror has been requested and requirements are met, the prepare_download_mirror Role is then included
.. Temp directories, various Environment Variables for the Run, and other details are finally set before continuing
. The Cloud Playbook is then imported
.. If Cloud actions are indicated in this Run, the Role `cloudera.exe.sequence` is imported
.. Any Dynamic Inventory result is persisted to Static Inventory artefact for later use
.. If the Download Mirror is in Play, it is then populated and the cached files then persisted to the Download Mirror cache file for later use
. If a Teardown has been requested, the clean_dynamic_inventory Role is run
. Then the preparation steps for a Cluster deployment are run if a Cluster definition was found during init’s parsing of the Definition
.. The static inventory artefact is loaded, if found (including when just created)
.. The Download Mirror URLs are injected from the cache artefact, if requested
.. Necessary facts for the Cluster deployment are distributed to the Inventory based on the Run thus far
. If a Cluster is defined, the Cluster Playbook is then imported
. The Application Playbook is imported from the Definition directory for any post-run tasks

==== Cloud Deployment Sequencing

The https://github.com/cloudera-labs/cloudera-deploy/blob/main/cloud.yml[cloud.yml] Playbook in Cloudera-Deploy acts as a wrapper for steps that are taken against Public Cloud Infrastructure.

NOTE: cloud.yml is expected be called within the main.yml workflow, and therefore expects that certain Ansible Variables are available. If you wish to modify or re-use behavior, you are advised to trace the Tasks from the start of main.yml to understand the process.

.The following steps are included:

. The `cloudera.exe.sequence` Role is imported if Cloud Infra or CDP Public Cloud included in the Definition
. If Dynamic Inventory has been created in the cloud steps, the details about it are persisted
. If the Download Mirror is in the Definition, it is prepared and the target files are downloaded to it
. The Download Mirror cache is then updated if any new downloads have been completed

==== Cluster Deployment Sequencing

The https://github.com/cloudera-labs/cloudera-deploy/blob/main/cluster.yml[cluster.yml] Playbook controls the sequence of Plays which deploy a CDP Base cluster, or legacy CDH cluster.

NOTE: cluster.yml is expected be called within the main.yml workflow, and therefore expects that certain Ansible Variables are available. If you wish to modify or re-use behavior, you are advised to trace the Tasks from the start of main.yml to understand the process.

It is a long Playbook at ~580 lines, and has many tags to activate or skip plays. There are Tags which control full sequences of Plays, specifically those which trigger a `full_cluster` build, a `default_cluster` build, or `teardown_all` to remove the deployed Clusters (and differs from the very top level `teardown` tag for Cloudera Deploy)

Tags to trigger individual Plays are best sourced from reading the individual Plays in cluster.yml

The main reason it is broken up into so many plays is that different tasks must happen in a particular sequence on a particular host or set of hosts at any given point. You are advised not to reorder the Plays unless you are quite familiar with the process.

There are a number of 'blocks' of Plays in the Playbook to help the user orient themselves, they are annotated in comments.

.The general Deployment sequence is:
. `Init Cluster Deployment Tasks`
* Prepare any local directories or other items needed on the Controller
* Generally required where files from the nodes need to come back to the Controller for the user to do something with, like handling certificate signing requests
. `Check Inventory Connectivity`
* Validate that the Inventory given to the Ansible Controller can actually be connected to
* Then collects the Ansible Facts from this inventory for use later
. `Verify Inventory and Definition`
* We run various validation steps on the Inventory and the Cluster Definition to attempt to fail early if known misconfigurations are present
* If a node is designated to host a `custom_repo`, then the Play `Install custom parcel repository` is run at this point so that the Repo may be populated with the various files and then validated against the provided Definition
. `Prepare Nodes`
* Having validated our Definition and prepared our Run, we now move onto preparing the nodes that will form the cluster
* We first apply the OS prerequisites such as huge pages and reconfiguring selinux
* We then create any local user accounts needed on the nodes
* The JDK is installed on the relevant nodes
* Depending on which Database is Defined for Cloudera Manager, the clients and other prequisites for that Database will be prepared
. `Create Cluster Service Infrastructure`
* If you've designated nodes for KDC, Certificate Authority or HAProxy, those nodes will be prepared
. `Prepare TLS`
* Having prepared the ca_server for hosting the Certificate Authority above, this section then prepares the TLS keystores and truststores from the certificates if necessary
. `Install Cluster Service Infrastructure`
* Having completed the necessary OS and other prereqs, we are now ready to start installing services
* First the selected RDBMS is installed on the `db_server` node if required by the Definition
* Then we setup the specific TLS setup for NiFi if it is required
. `Install Cloudera Manager`
* First the Cloudera Manager Daemons are installed
* Then the Cloudera Manager Server is installed
* If a Cloudera License is included in the Definition, it will be applied to the server, otherwise it will remain in Trial mode
* Then the Cloudera Manager agents are installed
* If TLS is enabled, Cloudera Manager is configured for TLS using the previously prepared configs
* Finally in the CM install, the various agent, server and auth configurations are layered in
. `Prepare Security`
* At this stage, if Auto-TLS is enabled in the Definition, it is applied to Cloudera Manager
* We then complete the Kerberos configuration for Cloudera Manager and the Cluster if necessary
. `Install Cluster`
* Next we move onto installing the Cluster within Cloudera Manager
* Firstly, we restart the CM Agents and ensure they are up and heartbeating
* We then install and start the Cloudera Manager Management Service
* If a `custom_repo` is in the Inventory, we then check for any parcels which could be preloaded to Cloudera Manager to speed up launch time
* Finally in this section, we deploy the Clusters listed in the Definition
* This involves importing the Cluster Template, which can take a long time
. `Setup HDFS Encryption`
* If KTS and or KMS are in the Inventory ahd Definition, they will now be configured
* Finally Client Configs are refreshed to fix up any stale entries

.The general Teardown sequence is:
* Note that the Teardown Plays follow the Deployment plays in the Playbook, so you'll see a lot of skipped plays when calling these specific teardowns
. If you have Tagged a `ca_server` teardown, it is the first to be run
. Then, if TLS is in your Definition, it will be cleaned up
. Finally, if you have Tagged to teardown one or more or all of the Clusters in Cloudera Manager, they will be removed

NOTE: There are no Teardown options for the stages preceding cluster deployment, as it is assumed that you would simply reinitialise the nodes and rerun a deployment to ensure a clean build.

[[cdDynamicInventory]]
==== Dynamic Inventory

The Dynamic Inventory implementation in cloudera-deploy is primarily intended as an aid to automated testing or trial deployments. It allows the user to provide a template inventory file, which is then converted into a number of VMs on the infra platform, which are in turn used within the same run as inventory for the CDP Private Cloud Base deployment, without any further interaction from the user.

Note that this is not necessarily a typical approach, where a user may prefer to create infrastructure using Terraform and then supply Ansible with a static inventory file. Or a user may wish to use a traditional Ansible dynamic inventory plugin. This functionality within Cloudera-Deploy is designed to make it abundantly easy for users new to these technologies to get going with minimal initial learning.

Presently this feature is only implemented for AWS, though extending each of the code sections for any other infrastructure provider is quite possible if the process below is understood. It is one of the more complex workflows in cloudera-deploy simply because it touches on several Roles and collections in order to avoid repetition and each segment of the work involved being executed with other similar workflows.

The process is deliberately broken up into sections with artefact files at key points to allow the user to substitute their own process or inventory at any given stage. It also makes testing changes a lot easier, and it may then be replaced using Terraform or some other provider at your convenience.

NOTE: Dynamic Inventory uses AMIs from the AWS Marketplace, it is necessary for someone to have subscribed to the selected image at least once within the account for it to work. If you are the first person to run this in an account you will get an error and a link to activate the subscription, then you can re-run to complete the deployment. Yay idempotence, as this can only be done in the GUI.

.The meta structure specific to this feature is as follows:

. Include an inventory_template.ini in your Definition path, which describes the shape of the infrastructure required for your CDP Base Cluster.
. Use a supported infra provider, currently this is only AWS
. When the main.yml playbook is run, the cloudera-deploy/init Role will pick up and process the template to determine validity and the number of hosts needed for the deployment.
.. Note that the name of the template file may be modified
.. Then existence of the template file is checked
.. We then check if a static inventory has been supplied with the -i switch, and do not process the dynamic inventory if it is set
.. If the above condition is satisfied, we then use the Ansible ‘refresh inventory’ command with the template included in the inventory dir, this is a tasklist called ‘refresh_inventory’.
.. Note that this is a minor hack as traditional Ansible usage would recommend against dynamic inventory refresh mid run, but we use it here in the name of NUX.
.. We then check some simple compliance details within the template, and if it passes we count the number of hosts required in the template and add it to the ‘globals’ variables as ‘vm.count’, which is in turn used later to generate the required host infrastructure.
.. We then remove the dynamic inventory template from the Ansible Inventory, and refresh the inventory again.
.. Note: We use this process because Ansible already has the logic to process the inventory template and give us a host count from it.
. Next, the main.yml playbook calls on the cloud.yml playbook, which in turn calls the sequence Role in the `cloudera.exe` Collection to run all of the Public Cloud tasks for creating the necessary infrastructure for the deployment.
.. The defaults used by `cloudera.exe` for creating the Dynamic Inventory are https://github.com/cloudera-labs/cloudera.exe/blob/main/roles/infrastructure/defaults/main.yml[here in the role defaults]
.. Note: that `globals.vm.count` from earlier is consumed here, along with naming, ssh, storage, etc. pieces.
.. Additional defaults are set from a https://github.com/cloudera-labs/cloudera.exe/blob/main/roles/infrastructure/vars/main.yml[vars] file in this Role, where we hold pickers for things like storage, vm, OS type, etc. particular to that Infrastructure provider. All of which may be overridden.
.. Initialization of necessary information is done in initialize_setup_aws in the same Role, here we get the correct AMI to use for VM deployment, and then the host connectivity information Ansible needs for inventory population
.. Then in the main setup.yml file within this infrastructure Role, we run the infra provider-specific ‘compute’ task list
... The compute task list first ensures the correct number of VMs is created with the given parameters. This task does a lot of work for us using the Infra provider’s own Ansible modules.
... It then prepares the `$$infra__dynamic_inventory_host_entries$$` variable with a list of the necessary connection information for each new host in a format that Ansible understands, which is a combination of the generic connector information in `$$infra__dynamic_inventory_connectors$$` (ansible_user, ssh private key file) and the VM-specific information (private FQDN as node name, Public FQDN as ansible_host)
+
NOTE: This connectivity information is very important for Ansible to build clusters correctly on AWS because AWS uses a reverse-proxy for DNS. Other infra providers would probably only require a much simpler set of connectivity information.
. Next, the cloud.yml playbook will run a post-processing check if dynamic inventory VMs have been created by checking the `$$infra__dynamic_inventory_host_entries$$` variable, and if it and the template are present it will merge them into a `static_inventory.ini` artefact in the Definition directory using the cloudera-deploy/persist_dynamic_inventory tasklist. This artifact is later used by the cluster.yml playbook.
+
NOTE:  This essentially picks up the connection entry for each host and replaces the template entry on a 1:1 basis, look at the example above to see how it works.
. Back in the initial main.yml, we check if a static inventory artefact is presented. If the user created their own, or used the preceding dynamic inventory process to generate a static inventory artefact, this process is then triggered, thus allowing both options. This once again uses the ‘refesh_inventory’ tasklist to inject the static_inventory artefact (regardless of how it was created) into the Ansible Inventory ready for a Cluster deployment.
. The cluster.yml playbook is then called, which will use whatever inventory is present to run the cluster deployment according to the rest of the configuration and tags presented. In this case it will pick up the static inventory artefact, produced by thi dynamic inventory process, and deploy the cluster.
